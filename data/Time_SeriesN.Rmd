---
title: "Term Project"
author: "Vikas Vicraman, Hetvi Dave, Nakul Nair"
date: "`r Sys.Date()`"
output:
  rmdformats::html_docco:
    highlight: kate
    toc: true
    toc_depth: 3
---

```{r knitr_init, echo=FALSE, cache=FALSE}
# DO NOT edit this block
knitr::opts_chunk$set(
  cache=TRUE,
  comment=NA,
  message=FALSE,
  warning=FALSE,
  fig.width=12,
  fig.height=7
)
```

#Load in packages
```{r packages, message=FALSE}
if(!require(pacman)) install.packages('pacman')
pacman::p_load(dplyr, locfit, ggplot2, mapproj, readr, ggthemes, viridis, reshape2, cowplot, gstat, sp, automap, data.table, lubridate, forecast)
```


# What is the Data?

Our data set contains information on the chemicals released by  a range of industries from 1986 to 2016. While the data is highly nuanced we want to examine chemical release by the petroleum industry in Texas. We have data on the types of chemicals and the medium through which they are released but to begin with, we will examine the total cehmical release. 

To provide further clarification, in our data set, the petroleum industry relates to:

"The Petroleum and Coal Products Manufacturing subsector is based on the transformation of crude petroleum and coal into usable products. The dominant process is petroleum refining that involves the separation of crude petroleum into component products through such techniques as cracking and distillation. In addition, this subsector includes establishments that primarily further process refined petroleum and coal products and produce products, such as asphalt coatings and petroleum lubricating oils."

We intend to relate this to the data on the production of crude oil in Texas, our logic being that it should be a good predictor as more petroleum manufacturing should happen when more petroleum has been extracted and vice versa. 

# Data Exploration

This data set is vast and highly nuanced, in order to approach the analysis methodologically, we start by only considering the most simple variables. Our goal is to examine how the total chemical release varies both by location and by time, attempt to fit a model to this and then test the success.

##Reading in and combining all the data files
```{r read, message=FALSE}
# get the names of the csv files in your current directory
file_names = list.files(pattern = "TRI")  

# for every name you found go and read the csv with that name 
# (this creates a list of files)
import_files = lapply(file_names, read.csv, stringsAsFactors = FALSE)

# append those files one after the other (collapse list elements to one dataset) and save it as d
df = do.call(rbind, import_files)

```

```{r}
head(df)
```


```{r}
dat <- df %>%
  select("YEAR", "LATITUDE","LONGITUDE", "INDUSTRY_SECTOR", "CHEMICAL", "CARCINOGEN", "TOTAL_RELEASES", "UNIT_OF_MEASURE", "CLASSIFICATION") %>%
  filter(INDUSTRY_SECTOR == "Petroleum")

colnames(dat)[colnames(dat)=="LONGITUDE"] <- "lon"
colnames(dat)[colnames(dat)=="LATITUDE"] <- "lat"
names(dat)[1:length(dat)] <- tolower(names(dat)[1:length(dat)])
table(dat$lat)
```


## Energy Dataset

```{r}
#setwd("/Users/Nakul/Documents/Columbia /S2/Env Data Analysis/Final Project/toxicEnvt/")
df2 <- read_csv('Energy.csv') %>%
  setNames(tolower(names(.)))  %>% # variable names are lower case
  filter(year >= "1989" & year < "2017") %>% #Cutting the time series down to the same length as the toxic chemical release data 
  select("year", "crude oil production (mbbl)")
  
colnames(df2)[which(names(df2) == "crude oil production (mbbl)")] <- "oil_mbbl"
energy <- mutate(df2, oil_kg = oil_mbbl*14000)
energy <- energy[seq(dim(energy)[1],1),]
energy

```

## Crude oil production over time

```{r}
ggplot(energy, aes(x = year, y = oil_kg)) +
  geom_point()
```

# Analysis of Chemical Release by Time
```{r}
total_release_time <- dat %>%
  select("year", "total_releases", "unit_of_measure") %>%
  filter(unit_of_measure == "Pounds") %>%    #dioxins measured in grams, excluding these
  group_by(year) %>%
  summarise(release_sum = sum(total_releases * 0.453592))  #converting pounds to kg
tail(total_release_time)

```

## All chemicals released from start to end of data time period
```{r}
ggplot(total_release_time, aes(x = year, y = release_sum)) +
  geom_point()
```
Analysis: Major outlier is financial crisis
Why was it decreasing before the financial crisis
How much of the post 2009 increase is from returning to normal business and how much of it was a genuine increase in petroleum activity.
Remove data from 1986 and 1987

## Remoivng outliers

```{r}
release_time <- total_release_time %>%
  filter(year >= "1989")     #removing huge earlier outliers

ggplot(release_time, aes(x = year, y = release_sum)) +
  geom_point()

```

## Normalizing data and plotting together

```{r}

en <- data.frame(oil_mbbl = energy$oil_mbbl, oil_kg_n = energy$oil_kg/mean(energy$oil_kg), year = release_time$year, release_sum_n = release_time$release_sum/mean(release_time$release_sum))  #joining data into a data frame


melt_en <- melt(en, id = c("year", "oil_mbbl"))   #melting the predictions together
# 
 ggplot(data = melt_en, aes(x = year, y = value)) +
   geom_line(aes(color = variable)) 
   
  
    #separating plots based on prediction index
```


# Analysis of Chemical Release by Location - for reference, move to kriging file

```{r}
states <- map_data("state") 
texas <- subset(states, region %in% c("texas"))

texas_map <- ggplot(data = texas) + 
  geom_polygon(aes(x = long, y = lat, group = group), fill = "palegreen", color = "black") + 
  coord_fixed(1.3)
```


```{r}
release_loc <- dat %>%
  select("year", "total_releases", "unit_of_measure", "lon", "lat") %>%
  filter(unit_of_measure == "Pounds" & year >= "1989") %>% 
  
  mutate(total_release = total_releases * 0.453592)  #converting pounds to kg

texas_map + geom_point(aes(x=lon, y = lat, size = total_releases) , data = release_loc) + 
  scale_size_continuous(range=c(3, 10))+ 
  scale_color_viridis() +
  theme_map()
```
Analysis: Map displaying the variability of the total release by location


#Model 1: Autoregression

Now that we have explored both the time series' we want to fit a model to it. Here we will consider autoregression, where the total chemical release of a certain year at a certain location is a function of the regression at that location in a previous year and the total amount of petroleum produced that year. 

## Visualizing time series

```{r}
release_time <- total_release_time %>%
  filter(year >= "1989")     #removing huge earlier outliers

ggplot(release_time, aes(x = year, y = release_sum)) +
  geom_line()
```


## Stationarize time series

Clearly, our chemical release time series is not stationary. So before using autoregression, we attempt to make the series stationary

```{r}
time_series <- release_time %>%
  select("year", "release_sum")
release <- diff(log(time_series$release_sum))
year <- time_series$year[-1]

stationary_time <- data.frame(year, release, )

```

```{r}
plot(release ~ year, stationary_time, type = "l")
adf.test(stationary_time$release, alternative="stationary", k=0)
```
The null hypothesis for the adf test is that the time series has a unit root i.e. it is non stationary. As we can see, this is rejected and so we have a stationary process that we can autoregress.

## Differencing order
```{r}
acf2(stationary_time$release)
```
We can see that the ACF at lag 1 is negative, and overall the the ACF values are small and patternless. This justifies the order of differencing we have used.

## Choosing ARIMA parameters

```{r}
fitARIMA <- auto.arima(stationary_time$release, ic = "aic", trace = TRUE)
futurVal <- forecast(fitARIMA,h=5, level=c(99.5))
summary(futurVal)
plot(futurVal)
```

p is the number of autoregressive terms,
d is the number of nonseasonal differences needed for stationarity, and
q is the number of lagged forecast errors in the prediction equation.




