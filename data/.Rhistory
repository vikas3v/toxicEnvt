return(df)
}
gen_q1_data <- function(UNI){
# UNI: last 4 (numeric) digits of your UNI
require(dplyr)
set.seed(UNI)
year <- seq(1980, 2018, by=1/12)
y <- sin(pi / 180 * 365.25 * year)
conc <- runif(1, 0, 10) * exp(rnorm(length(y), y, abs(y)))
qc <- quantile(conc, 0.1)
for (i in 1:length(y))
conc[i]=max(conc[i],qc)
df <- tibble(year=year, conc_lead=conc)
qu <- round(quantile(conc, 0.95), 0)
print(paste("Is there a trend in the occurrence of Pb concentrations greater than", as.integer(qu)))
return(df)
}
gen_q1_data(2108)
# DO NOT edit this block
knitr::opts_chunk$set(
cache=TRUE,
comment=NA,
message=FALSE,
warning=FALSE,
fig.width=12,
fig.height=7
)
if(!require(pacman)) install.packages('pacman')
pacman::p_load(dplyr, locfit, ggplot2, mapproj, readr, ggthemes, viridis, reshape2, cowplot, gstat, sp, automap, data.table, lubridate, forecast)
# get the names of the csv files in your current directory
file_names = list.files(pattern = "TRI")
# for every name you found go and read the csv with that name
# (this creates a list of files)
import_files = lapply(file_names, read.csv, stringsAsFactors = FALSE)
# append those files one after the other (collapse list elements to one dataset) and save it as d
df = do.call(rbind, import_files)
head(df)
dat <- df %>%
select("YEAR", "LATITUDE","LONGITUDE", "INDUSTRY_SECTOR", "CHEMICAL", "CARCINOGEN", "TOTAL_RELEASES", "UNIT_OF_MEASURE", "CLASSIFICATION") %>%
filter(INDUSTRY_SECTOR == "Petroleum")
colnames(dat)[colnames(dat)=="LONGITUDE"] <- "lon"
colnames(dat)[colnames(dat)=="LATITUDE"] <- "lat"
names(dat)[1:length(dat)] <- tolower(names(dat)[1:length(dat)])
table(dat$lat)
#setwd("/Users/Nakul/Documents/Columbia /S2/Env Data Analysis/Final Project/toxicEnvt/")
df2 <- read_csv('Energy.csv') %>%
setNames(tolower(names(.)))  %>% # variable names are lower case
filter(year >= "1989" & year < "2017") %>% #Cutting the time series down to the same length as the toxic chemical release data
select("year", "crude oil production (mbbl)")
colnames(df2)[which(names(df2) == "crude oil production (mbbl)")] <- "oil_mbbl"
energy <- mutate(df2, oil_kg = oil_mbbl*14000)
energy <- energy[seq(dim(energy)[1],1),]
energy
ggplot(energy, aes(x = year, y = oil_kg)) +
geom_point()
total_release_time <- dat %>%
select("year", "total_releases", "unit_of_measure") %>%
filter(unit_of_measure == "Pounds") %>%    #dioxins measured in grams, excluding these
group_by(year) %>%
summarise(release_sum = sum(total_releases * 0.453592))  #converting pounds to kg
tail(total_release_time)
ggplot(total_release_time, aes(x = year, y = release_sum)) +
geom_point()
release_time <- total_release_time %>%
filter(year >= "1989")     #removing huge earlier outliers
ggplot(release_time, aes(x = year, y = release_sum)) +
geom_point()
en <- data.frame(oil_mbbl = energy$oil_mbbl, oil_kg_n = energy$oil_kg/mean(energy$oil_kg), year = release_time$year, release_sum_n = release_time$release_sum/mean(release_time$release_sum))  #joining data into a data frame
melt_en <- melt(en, id = c("year", "oil_mbbl"))   #melting the predictions together
#
ggplot(data = melt_en, aes(x = year, y = value)) +
geom_line(aes(color = variable))
#separating plots based on prediction index
states <- map_data("state")
texas <- subset(states, region %in% c("texas"))
texas_map <- ggplot(data = texas) +
geom_polygon(aes(x = long, y = lat, group = group), fill = "palegreen", color = "black") +
coord_fixed(1.3)
release_loc <- dat %>%
select("year", "total_releases", "unit_of_measure", "lon", "lat") %>%
filter(unit_of_measure == "Pounds" & year >= "1989") %>%
mutate(total_release = total_releases * 0.453592)  #converting pounds to kg
texas_map + geom_point(aes(x=lon, y = lat, size = total_releases) , data = release_loc) +
scale_size_continuous(range=c(3, 10))+
scale_color_viridis() +
theme_map()
release_time <- total_release_time %>%
filter(year >= "1989")     #removing huge earlier outliers
ggplot(release_time, aes(x = year, y = release_sum)) +
geom_line()
time_series <- release_time %>%
select("year", "release_sum")
release <- diff(log(time_series$release_sum))
year <- time_series$year[-1]
stationary_time <- data.frame(year, release)
plot(release ~ year, stationary_time, type = "l")
adf.test(stationary_time$release, alternative="stationary", k=0)
head(df)
dat <- df %>%
select("YEAR", "LATITUDE","LONGITUDE", "INDUSTRY_SECTOR", "CHEMICAL", "CARCINOGEN", "TOTAL_RELEASES", "UNIT_OF_MEASURE", "CLASSIFICATION") %>%
filter(INDUSTRY_SECTOR == "Petroleum")
colnames(dat)[colnames(dat)=="LONGITUDE"] <- "lon"
colnames(dat)[colnames(dat)=="LATITUDE"] <- "lat"
names(dat)[1:length(dat)] <- tolower(names(dat)[1:length(dat)])
table(dat$lat)
#setwd("/Users/Nakul/Documents/Columbia /S2/Env Data Analysis/Final Project/toxicEnvt/")
df2 <- read_csv('Energy.csv') %>%
setNames(tolower(names(.)))  %>% # variable names are lower case
filter(year >= "1989" & year < "2017") %>% #Cutting the time series down to the same length as the toxic chemical release data
select("year", "crude oil production (mbbl)")
colnames(df2)[which(names(df2) == "crude oil production (mbbl)")] <- "oil_mbbl"
energy <- mutate(df2, oil_kg = oil_mbbl*14000)
energy <- energy[seq(dim(energy)[1],1),]
energy
ggplot(energy, aes(x = year, y = oil_kg)) +
geom_point()
total_release_time <- dat %>%
select("year", "total_releases", "unit_of_measure") %>%
filter(unit_of_measure == "Pounds") %>%    #dioxins measured in grams, excluding these
group_by(year) %>%
summarise(release_sum = sum(total_releases * 0.453592))  #converting pounds to kg
tail(total_release_time)
ggplot(total_release_time, aes(x = year, y = release_sum)) +
geom_point()
release_time <- total_release_time %>%
filter(year >= "1989")     #removing huge earlier outliers
ggplot(release_time, aes(x = year, y = release_sum)) +
geom_point()
en <- data.frame(oil_mbbl = energy$oil_mbbl, oil_kg_n = energy$oil_kg/mean(energy$oil_kg), year = release_time$year, release_sum_n = release_time$release_sum/mean(release_time$release_sum))  #joining data into a data frame
melt_en <- melt(en, id = c("year", "oil_mbbl"))   #melting the predictions together
#
ggplot(data = melt_en, aes(x = year, y = value)) +
geom_line(aes(color = variable))
#separating plots based on prediction index
states <- map_data("state")
texas <- subset(states, region %in% c("texas"))
texas_map <- ggplot(data = texas) +
geom_polygon(aes(x = long, y = lat, group = group), fill = "palegreen", color = "black") +
coord_fixed(1.3)
release_loc <- dat %>%
select("year", "total_releases", "unit_of_measure", "lon", "lat") %>%
filter(unit_of_measure == "Pounds" & year >= "1989") %>%
mutate(total_release = total_releases * 0.453592)  #converting pounds to kg
texas_map + geom_point(aes(x=lon, y = lat, size = total_releases) , data = release_loc) +
scale_size_continuous(range=c(3, 10))+
scale_color_viridis() +
theme_map()
release_time <- total_release_time %>%
filter(year >= "1989")     #removing huge earlier outliers
ggplot(release_time, aes(x = year, y = release_sum)) +
geom_line()
time_series <- release_time %>%
select("year", "release_sum")
release <- diff(log(time_series$release_sum))
year <- time_series$year[-1]
stationary_time <- data.frame(year, release)
plot(release ~ year, stationary_time, type = "l")
adf.test(stationary_time$release, alternative="stationary", k=0)
if(!require(pacman)) install.packages('pacman')
pacman::p_load(dplyr, locfit, ggplot2, mapproj, readr, ggthemes, viridis, reshape2, cowplot, gstat, sp, automap, data.table, lubridate, forecast, tseries)
plot(release ~ year, stationary_time, type = "l")
adf.test(stationary_time$release, alternative="stationary", k=0)
acf2(stationary_time$release)
time_series <- release_time %>%
select("year", "release_sum")
release <- diff(log(time_series$release_sum))
year <- time_series$year[-1]
stationary_time <- data.frame(year, release)
plot(release ~ year, stationary_time, type = "l")
adf.test(stationary_time$release, alternative="stationary", k=0)
if(!require(pacman)) install.packages('pacman')
pacman::p_load(dplyr, locfit, ggplot2, mapproj, readr, ggthemes, viridis, reshape2, cowplot, gstat, sp, automap, data.table, lubridate, forecast, tseries, astsa)
acf2(stationary_time$release)
fitARIMA <- auto.arima(stationary_time$release, ic = "aic", trace = TRUE)
futurVal <- forecast(fitARIMA,h=5, level=c(99.5))
summary(futurVal)
plot(futurVal)
release_lag <- en %>%
mutate(lag_release = lag(release_sum_n, 1)) %>%
filter(!is.na(lag_release))
head(release_lag)
lm_mod <- lm(release_sum_n ~ lag_release + year + oil_kg_n, data = release_lag)
summary(lm_mod)
poly_mod <- lm(release_sum_n ~ #lag_release
+ year + oil_kg_n, data = release_lag)
summary(poly_mod)
plot(release_lag$year, release_lag$release_sum_n, xlab="Year", ylab="Release", main="Polynomial Fit")
lines(release_lag$year, predict(poly_mod))
fitARIMA <- arima(stationary_time$release, order = (1, 0, 0), ic = "aic", trace = TRUE)
fitARIMA <- arima(stationary_time$release, order = (1 0 0), ic = "aic", trace = TRUE)
fitARIMA <- arima(stationary_time$release, order = c(1, 0, 0), ic = "aic", trace = TRUE)
fitARIMA <- auto.arima(stationary_time$release, ic = "aic", trace = TRUE)
futurVal <- forecast(fitARIMA,h=5, level=c(99.5))
summary(futurVal)
plot(futurVal)
#fitARIMA <- auto.arima(stationary_time$release, ic = "aic", trace = TRUE)
fitARIMA <- arima(stationary_time$release, order = c(1, 0, 0))
order = c(1, 0, 0)
futurVal <- forecast(fitARIMA,h=5, level=c(99.5))
summary(futurVal)
plot(futurVal)
poly_mod <- lm(release_sum_n ~ lag_release + year + oil_kg_n, data = release_lag)
summary(poly_mod)
plot(release_lag$year, release_lag$release_sum_n, xlab="Year", ylab="Release", main="Polynomial Fit")
lines(release_lag$year, predict(poly_mod))
par(mfrow = c(2, 2)) # set up 2 by 2 plot
plot(poly_mod) # built-in diagnostic plots
par(mfrow = c(1, 1)) # go back to 1 by 1 plots (default)
poly_mod <- lm(release_sum_n ~ ploy(lag_release, 2) + poly(year, 2) + poly(oil_kg_n, 2), data = release_lag)
poly_mod <- lm(release_sum_n ~ poly(lag_release, 2) + poly(year, 2) + poly(oil_kg_n, 2), data = release_lag)
summary(poly_mod)
plot(release_lag$year, release_lag$release_sum_n, xlab="Year", ylab="Release", main="Polynomial Fit")
lines(release_lag$year, predict(poly_mod))
par(mfrow = c(2, 2)) # set up 2 by 2 plot
plot(poly_mod) # built-in diagnostic plots
par(mfrow = c(1, 1)) # go back to 1 by 1 plots (default)
poly_mod <- lm(release_sum_n ~ poly(lag_release, 3) + poly(year, 3) + poly(oil_kg_n, 3), data = release_lag)
summary(poly_mod)
plot(release_lag$year, release_lag$release_sum_n, xlab="Year", ylab="Release", main="Polynomial Fit")
lines(release_lag$year, predict(poly_mod))
par(mfrow = c(2, 2)) # set up 2 by 2 plot
plot(poly_mod) # built-in diagnostic plots
par(mfrow = c(1, 1)) # go back to 1 by 1 plots (default)
lm_mod <- lm(release_sum_n ~ lag_release + year + oil_kg_n, data = release_lag)
summary(lm_mod)
poly_mod <- lm(release_sum_n ~ poly(lag_release, 3) + poly(year, 3) + poly(oil_kg_n, 3), data = release_lag)
summary(poly_mod)
plot(release_lag$year, release_lag$release_sum_n, xlab="Year", ylab="Release", main="Polynomial Fit")
lines(release_lag$year, predict(poly_mod))
par(mfrow = c(2, 2)) # set up 2 by 2 plot
plot(poly_mod) # built-in diagnostic plots
par(mfrow = c(1, 1)) # go back to 1 by 1 plots (default)
poly_mod <- lm(release_sum_n ~ poly(lag_release, 2) + poly(year, 2) + poly(oil_kg_n, 2), data = release_lag)
summary(poly_mod)
poly_mod <- lm(release_sum_n ~ poly(lag_release, 3) + poly(year, 2) + poly(oil_kg_n, 2), data = release_lag)
summary(poly_mod)
poly_mod <- lm(release_sum_n ~ poly(lag_release, 3) + poly(year, 3) + poly(oil_kg_n, 2), data = release_lag)
poly_mod <- lm(release_sum_n ~ poly(lag_release, 3) + poly(year, ) + poly(oil_kg_n, 2), data = release_lag)
poly_mod <- lm(release_sum_n ~ poly(lag_release, 3) + poly(year, 2) + poly(oil_kg_n, 2), data = release_lag)
summary(poly_mod)
poly_mod <- lm(release_sum_n ~ poly(lag_release, 3) + poly(year, 3) + poly(oil_kg_n, 2), data = release_lag)
summary(poly_mod)
poly_mod <- lm(release_sum_n ~ poly(lag_release, 3) + poly(year, 3) + poly(oil_kg_n, 3), data = release_lag)
summary(poly_mod)
poly_mod <- lm(release_sum_n ~ poly(lag_release, 3) + poly(year, 3) + poly(oil_kg_n, 4), data = release_lag)
summary(poly_mod)
poly_mod <- lm(release_sum_n ~ poly(lag_release, 3) + poly(year, 4) + poly(oil_kg_n, 4), data = release_lag)
summary(poly_mod)
poly_mod <- lm(release_sum_n ~ poly(lag_release, 4) + poly(year, 4) + poly(oil_kg_n, 4), data = release_lag)
summary(poly_mod)
plot(release_lag$year, release_lag$release_sum_n, xlab="Year", ylab="Release", main="Polynomial Fit")
lines(release_lag$year, predict(poly_mod))
par(mfrow = c(2, 2)) # set up 2 by 2 plot
plot(poly_mod) # built-in diagnostic plots
par(mfrow = c(1, 1)) # go back to 1 by 1 plots (default)
poly_mod <- lm(release_sum_n ~ poly(lag_release, 3) + poly(year, 3) + poly(oil_kg_n, 3), data = release_lag)
summary(poly_mod)
par(mfrow = c(2, 2)) # set up 2 by 2 plot
plot(poly_mod) # built-in diagnostic plots
par(mfrow = c(1, 1)) # go back to 1 by 1 plots (default)
par(mfrow = c(2, 2)) # set up 2 by 2 plot
plot(lm_mod) # built-in diagnostic plots
par(mfrow = c(1, 1)) # go back to 1 by 1 plots (default)
#fitARIMA <- auto.arima(stationary_time$release, ic = "aic", trace = TRUE)
fitARIMA <- arima(stationary_time$release, order = c(1, 0, 0))
order = c(1, 0, 0)
futurVal <- forecast(fitARIMA,h=10, level=c(99.5))
summary(futurVal)
plot(futurVal)
#fitARIMA <- auto.arima(stationary_time$release, ic = "aic", trace = TRUE)
fitARIMA <- arima(stationary_time$release, order = c(1, 0, 1))
order = c(1, 0, 0)
futurVal <- forecast(fitARIMA,h=10, level=c(99.5))
summary(futurVal)
plot(futurVal)
#fitARIMA <- auto.arima(stationary_time$release, ic = "aic", trace = TRUE)
fitARIMA <- arima(stationary_time$release, order = c(1, 0, 0))
order = c(1, 0, 0)
futurVal <- forecast(fitARIMA,h=10, level=c(99.5))
summary(futurVal)
plot(futurVal)
#fitARIMA <- auto.arima(stationary_time$release, ic = "aic", trace = TRUE)
fitARIMA <- arima(stationary_time$release, order = c(1, 0, 0))
order = c(1, 0, 0)
futurVal <- forecast(fitARIMA,h=2, level=c(99.5))
summary(futurVal)
plot(futurVal)
#fitARIMA <- auto.arima(stationary_time$release, ic = "aic", trace = TRUE)
fitARIMA <- arima(stationary_time$release, order = c(1, 0, 0))
order = c(1, 0, 0)
futurVal <- forecast(fitARIMA,h=3, level=c(99.5))
summary(futurVal)
plot(futurVal)
#fitARIMA <- auto.arima(stationary_time$release, ic = "aic", trace = TRUE)
fitARIMA <- arima(stationary_time$release, order = c(1, 0, 0))
order = c(1, 0, 0)
futurVal <- forecast(fitARIMA,h=4, level=c(99.5))
summary(futurVal)
plot(futurVal)
#fitARIMA <- auto.arima(stationary_time$release, ic = "aic", trace = TRUE)
fitARIMA <- arima(stationary_time$release, order = c(1, 0, 0))
order = c(1, 0, 0)
futurVal <- forecast(fitARIMA,h=5, level=c(99.5))
summary(futurVal)
plot(futurVal)
# DO NOT edit this block
knitr::opts_chunk$set(
cache=TRUE,
comment=NA,
message=FALSE,
warning=FALSE,
fig.width=12,
fig.height=7
)
if(!require(pacman)) install.packages('pacman')
pacman::p_load(dplyr, ggplot2, mapproj, readr, ggthemes, viridis, tidyr, reshape2, gridExtra)
cty <- read_csv('cty.csv') %>%
select(lon, lat, Uppm)
glimpse(cty)
ggplot(cty, aes(x=lon, y=lat)) +
geom_point(aes(size=Uppm)) +
scale_size_continuous(range=c(0.1, 1))
ggplot(cty, aes(x=lon, y=lat)) +
geom_point(aes(size=Uppm, color=Uppm)) +
scale_size_continuous(range=c(0.1, 1)) +
coord_map(projection="lambert", parameters = c(25, 50)) +
scale_color_viridis() +
theme_map()
ggplot(cty, aes(x=lon, y=lat)) +
geom_point(aes(alpha=Uppm, color=Uppm)) +    #alpha is opacity
scale_size_continuous(range=c(0.1, 1)) +
coord_map(projection="albers", parameters = c(25, 50)) +
scale_color_viridis() +
theme_map()
summary(cty)
qqnorm(cty$Uppm, main = "Normal Q-Q Plot of Radon Concentrations")
qqline(cty$Uppm)
hist(cty$Uppm, main = "Histogram of Radon Concentrations")
cty1 <- filter(cty, Uppm>0)
summary(cty1$Uppm)
qqnorm(cty1$Uppm, main = "Normal Q-Q Plot of Radon Concentrations")
qqline(cty1$Uppm)
hist(cty1$Uppm, main = "Histogram of Radon Concentrations")
cty2 <- cty1 %>%
mutate(log_Uppm = log(Uppm))
glimpse(cty2)
hist(cty2$log_Uppm, main = "Histogram of Radon Concentrations")
ks.test (cty2$Uppm, 'pnorm', mean(cty2$Uppm), sd(cty2$Uppm))
ks.test (cty2$log_Uppm, 'pnorm', mean(cty2$log_Uppm), sd(cty2$log_Uppm))
plot(density(cty2$Uppm, bw="nrd"))   #A kernel density estimate is similar to a smoothed histogram
x_sorted <- seq(min(cty2$Uppm), max(cty2$Uppm), length.out = 250)
p <- dnorm(x_sorted, mean(cty2$Uppm), sd(cty2$Uppm));
lines(x_sorted, p, col="red", lty=2)
lm1 <- lm(Uppm ~ lon + lat, data = cty1)
summary(lm1)
par(mfrow = c(2, 2)) # set up 2 by 2 plot
plot(lm1) # built-in diagnostic plots
par(mfrow = c(1, 1)) # go back to 1 by 1 plots (default)
lm2 <- lm(Uppm ~ poly(lon, 4, raw=TRUE) + poly(lat, 5, raw=TRUE), data = cty1)
summary(lm2)
BIC(lm2)
plot_df <- cty1
plot_df$residual <- lm2$residuals
require(ggplot2)
plot_df %>%
ggplot(aes(x=lon, y=lat)) +
geom_point(aes(color=residual))
cty3 <- cty2 %>%
mutate(resid1 = residuals(lm1), resid2 = residuals(lm2))
head(cty3)
cty3 %>%
gather(-lon, -lat, -Uppm, -log_Uppm, key = "var", value = "value") %>%
ggplot(aes(x=lon, y=lat)) +
geom_point(aes(color = value)) +    #alpha is opacity
scale_size_continuous(range=c(0.1, 1)) +
coord_map(projection="albers", parameters = c(25, 50)) +
facet_wrap(~ var) +
scale_color_viridis() +
theme_map()+theme(legend.position="bottom")
# k = 15 for this problem
knnreg <- function(x_train, y_train, x_test, k, weights=NULL){
# Implement a K-Nearest-Neighbor Regression
#
# The predtion is carried out using a kernel regression approach with random
# sampling. See in-class notes.
#
# Inputs:
#   x_train: training data predictors
#   y_train: training data observed predictand - dependent variable training (answers)
#   x_test: test data predictors
#   k: the number of nearest-neighbors to use
#   weights: weights to assign to each observation (default NULL)
# Returns:
#   y_test: nearest neighbor estimate of predictand given x_test - dependent variable results
#
# Original by Upmanu Lall, Columbia University
# Updated by James Doss-Gollin, Columbia University
# Get the names of y_test
y_names <- names(y_test)
# Convert all inputs to matrix format
x_train <- as.matrix(x_train)
x_test <- as.matrix(x_test)
y_train <- as.matrix(y_train)
# Make sure the input dimensions are correct
n_train <- nrow(x_train)
if (nrow(y_train) != n_train) stop('x_train and y_train have different number of rows')
n_predictors <- ncol(x_train)
if (ncol(x_test) != n_predictors) stop('x_train and x_test have different number predictors')
n_test <- nrow(x_test)
n_predictand <- ncol(y_train)
# set weights if none are given
if (is.null(weights)) weights <- rep(1, n_predictors)   #n_predictors is the number of columns of the x training set
# Initialize the y_test matrix
y_test <- matrix(NA, nrow = n_test, ncol = n_predictand)
# Loop through each test data point:
for (n in 1:n_test){
# compute the distance from our test point to each training point
distance <- matrix(NA, nrow = n_train, ncol = n_predictors)
for (i in 1:n_predictors){
distance[, i] <- 100 * weights[i] * (x_train[, i]- x_test[n, i])^2
}
distance <- rowSums(distance, na.rm=TRUE)
distance_rank <- rank(distance)
# get the positions of the neighbors for each prediction vector
neighbor_idx <- rep(NA, n_test)
neighbor_idx[rank(distance_rank, ties.method='first')] <- seq(1, n_train)
neighbor_idx <- neighbor_idx[1:k] # keep only k nearest neighbors!
# We make the prediction by taking the average value of the k nearest
# neighbors, weighted by the *rank* of their distance from the
# test point. Here "distance" means Euclidean distance.
sum_dist_rank <- sum(distance_rank[1:k]^(-1)) # normalizing factor
dist_prob <- (distance_rank[1:k] ^ (-1)) / sum_dist_rank # sampling probabilities
sampled_indices <- sample(neighbor_idx, 1000, replace=T, prob=dist_prob) # re-sample the indices
# make the predictions
y_sampled <- y_train[neighbor_idx]
if (n_predictand == 1) {
y_test[n, ] <- mean(y_sampled, na.rm = TRUE)
} else{
y_test[n, ] <- colMeans(y_sampled, na.rm = TRUE)
}
}
# convert to data frame
y_test <- data.frame(y_test)
names(y_test) <- y_names
return(y_test)
}
# Need to split the data up into the testing and training sets in order to run the regression
set.seed(123)
cty_knn <- cty1 %>%
as_tibble() %>%       #convert dataframe into a tibble
mutate(index = sample(1:n()))  #randomly assigning an index to each observation
cty_train <- cty_knn %>% filter(index %% 5 != 0)
cty_test <- cty_knn %>% filter(index %% 5 == 0) # splitting data up into training and testing
y_train <- cty_train %>% select(Uppm)
x_train <- cty_train %>% select(lon, lat)
y_test <- cty_test %>% select(Uppm)
x_test <- cty_test %>% select(lon, lat)
y_pred <- knnreg(x_train = x_train, y_train = y_train, x_test = x_test, k = 15)
plot(y_test$Uppm, y_pred$Uppm, xlab='Actual', ylab='Predicted')
abline(0, 1, col='red', lty=2)
summary(y_pred)
#sample_size <- floor(0.75 * nrow(cty1))   #splitting the data up into training and testing as 75/25
#cty_train <- cty1 %>%
#  sample(seq(nrow()), size = sample_size)
cv_cty <- function(n_folds, n_neigh){
# n_folds: number of folds to use
# n_neigh: number of nearest neighbors
cty_knn <- cty_knn %>%
mutate(fold = (index %% n_folds) + 1)   #+1 so fold doesn't have a 0 index
# for each sub-fold create a data frame of true and predicted radon concentrations
results_df <- vector('list', length = n_folds) # initializing a variable of length K to hold the MSE
for (k_i in 1:n_folds){                         # splitting the data into K chunks of equal size
x_test <- filter(cty_knn, fold == k_i) %>% select(lon, lat)   #test data is the fold selected by the iteration number
x_train <- filter(cty_knn, fold != k_i) %>% select(lon, lat)   #all other folds are used for training purposes
y_train <- filter(cty_knn, fold != k_i) %>% select(Uppm)
y_test <- filter(cty_knn, fold == k_i) %>% select(Uppm)
y_hat <- knnreg(x_train = x_train, y_train = y_train, x_test = x_test, k = n_neigh)  #trains and then tests the data
results_df[[k_i]] <- data.frame(y_hat = y_hat$Uppm, y_true = y_test$Uppm)
}
# convert our list of data frames to a single data frame
results_df <- bind_rows(results_df)
# get MSE
squared_error <- (results_df$y_hat - results_df$y_true) ^ 2     #calculating MSE
return(mean(sum(squared_error)))
}
n_nearest_neigh <- 2:10   #checking the results for using
nn_mse <- rep(NA, length(n_nearest_neigh))   #empty list to store errors
for (i in 1:length(n_nearest_neigh)){   #
nn_mse[i] <- cv_cty(n_neigh=n_nearest_neigh[i], n_folds=15)
}
plot(n_nearest_neigh, nn_mse)
cv_cty1 <- function(n_folds, n_neigh){
# n_folds: number of folds to use
# n_neigh: number of nearest neighbors
cty_knn <- cty_knn %>%
mutate(fold = (index %% n_folds) + 1)   #+1 so fold doesn't have a 0 index
# for each sub-fold create a data frame of true and predicted radon concentrations
results_df <- vector('list', length = n_folds) # initializing a variable of length K to hold the MSE
for (k_i in 1:n_folds){                         # splitting the data into K chunks of equal size
x_test <- filter(cty_knn, fold == k_i) %>% select(lon, lat)   #test data is the fold selected by the iteration number
x_train <- filter(cty_knn, fold != k_i) %>% select(lon, lat)   #all other folds are used for training purposes
y_train <- filter(cty_knn, fold != k_i) %>% select(Uppm)
y_test <- filter(cty_knn, fold == k_i) %>% select(Uppm)
y_hat <- knnreg(x_train = x_train, y_train = y_train, x_test = x_test, k = n_neigh)  #trains and then tests the data
results_df[[k_i]] <- data.frame(y_hat = y_hat$Uppm, y_true = y_test$Uppm)
}
# convert our list of data frames to a single data frame
results_df <- bind_rows(results_df)
# get MSE
resid <- results_df$y_true - results_df$y_hat     #calculating MSE
return(resid)
}
